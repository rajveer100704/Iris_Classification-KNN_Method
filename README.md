ðŸŒ¸ K-Nearest Neighbors (KNN) Classification on Iris Dataset
ðŸ“Œ Overview

This project demonstrates K-Nearest Neighbors (KNN) classification using the classic Iris dataset. The goal is to explore how instance-based learning works, understand the role of distance metrics, and evaluate model performance across different values of K.

ðŸ”§ Tools & Libraries

Python

Pandas, NumPy

Scikit-learn

Matplotlib, Seaborn

ðŸ“Š Project Workflow

Dataset Loading â€“ Iris dataset from Kaggle.

Preprocessing â€“ Removed unnecessary columns and normalized features using StandardScaler.

Model Training â€“ Implemented KNN with varying k values.

Evaluation â€“ Measured accuracy, confusion matrix, and classification report.

Visualization â€“ Plotted accuracy vs. K and decision boundaries for 2D features.

ðŸ“ˆ Results

Best accuracy achieved around K = (check results in notebook).

Decision boundary plots highlight the separation between Iris species.

Normalization significantly improved distance-based classification performance.

ðŸ§  Key Learnings

Importance of feature scaling in distance-based algorithms.

Trade-off in selecting K (small K â†’ sensitive to noise, large K â†’ smoother boundary).

KNN works naturally for multi-class classification.

Distance metric choice impacts results.
